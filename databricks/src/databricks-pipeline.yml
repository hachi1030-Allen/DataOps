trigger:
- main

pool:
  vmImage: 'ubuntu-latest'

stages:
  - stage: artifacts
    displayName: Prepare Artifacts
    jobs:
      - job:
        steps:

        - task: CopyFiles@2
          inputs:
            SourceFolder: '$(Build.SourcesDirectory)/databricks/src'
            Contents: | # databricks supported extensions: https://docs.databricks.com/notebooks/notebooks-manage.html#notebook-external-formats
              **/*.py
              **/*.sql
              **/*.scala
              **/*.sc
              **/*.r
              **/*.html
              **/*.ipynb
              **/*.Rmd
            TargetFolder: '$(Build.ArtifactsStagingDirectory)/src'
        
        - task: CopyFiles@2
          inputs:
            SourceFolder: '$(Build.SourcesDirectory)/databricks/iac'
            Contents: '**'
            TargetFolder: '$(Build.ArtifactsStagingDirectory)/iac'
        
        - publish: $(Build.ArtifactsStagingDirectory)
          artifact: databricks

  - stage: DeployTest
    displayName: Deploy - Test
    jobs:
      - deployment:
        displayName: Databricks Deploy
        environment: dev
        strategy:
          runOnce:
            preDeploy:
              steps:
                - download: current
                  artifact: databricks
                
                - powershell: |
                    Get-ChildItem $(Pipeline.Workspace) -recurse
                
                - task: AzureResourceManagerTemplateDeployment@3
                  inputs:
                    deploymentScope: 'Resource Group'
                    azureResourceManagerConnection: 'Isha_ABI'
                    subscriptionId: 'c03d3356-a4b8-48aa-bdf0-d5ace508a0a0'
                    action: 'Create Or Update Resource Group'
                    resourceGroupName: $(Test-RG)
                    location: 'West Europe'
                    templateLocation: 'Linked artifact'
                    csmFile: '$(Pipeline.Workspace)/databricks/iac/databricks.json'
                    deploymentMode: 'Incremental'
                    
            deploy:
              steps:
              - task: UsePythonVersion@0
                displayName: Configuring Python Version
                inputs:
                  versionSpec: '3.x'
                  addToPath: true
                  architecture: 'x64'

              - powershell: |
                  Write-Output "[AZDO]" | Out-File ~/.databrickscfg -Encoding ASCII
                  Write-Output "host = $(databricks.host)" | Out-File ~/.databrickscfg -Encoding ASCII -Append
                  Write-Output "token = $(databricks.token)" | Out-File ~/.databrickscfg -Encoding ASCII -Append

                  pip3 install --upgrade databricks-cli
                displayName: configuring databricks

              - powershell: |                  
                  databricks workspace import_dir -o --profile AZDO $(Pipeline.Workspace)/databricks/src/ /src
                displayName: deploying notebooks

  - stage: DeployProd
    displayName: Deploy - Production
    jobs:
      - deployment:
        displayName: Databricks Deploy
        environment: prod
        strategy:
          runOnce:
            preDeploy:
              steps:
                - download: current
                  artifact: databricks
                
                - powershell: |
                    Get-ChildItem $(Pipeline.Workspace) -recurse
                
                - task: AzureResourceManagerTemplateDeployment@3
                  inputs:
                    deploymentScope: 'Resource Group'
                    azureResourceManagerConnection: 'Isha_ABI'
                    subscriptionId: 'c03d3356-a4b8-48aa-bdf0-d5ace508a0a0'
                    action: 'Create Or Update Resource Group'
                    resourceGroupName: $(Prod-RG)
                    location: 'West Europe'
                    templateLocation: 'Linked artifact'
                    csmFile: '$(Pipeline.Workspace)/databricks/iac/databricks.json'
                    deploymentMode: 'Incremental'
                    
            deploy:
              steps:
              - task: UsePythonVersion@0
                displayName: Configuring Python Version
                inputs:
                  versionSpec: '3.x'
                  addToPath: true
                  architecture: 'x64'

              - powershell: |
                  Write-Output "[AZDO]" | Out-File ~/.databrickscfg -Encoding ASCII
                  Write-Output "host = $(prod.databricks.host)" | Out-File ~/.databrickscfg -Encoding ASCII -Append
                  Write-Output "token = $(prod.databricks.token)" | Out-File ~/.databrickscfg -Encoding ASCII -Append

                  pip3 install --upgrade databricks-cli
                displayName: configuring databricks

              - powershell: |                  
                  databricks workspace import_dir -o --profile AZDO $(Pipeline.Workspace)/databricks/src/ /src
                displayName: deploying notebooks